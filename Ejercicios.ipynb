{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be213434",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"> Escuela Técnica Superior de Ingeniería Informática</p>\n",
    "<p style=\"text-align: center\">Universidad de Sevilla</p>\n",
    "<p style=\"text-align: center\">Síntesis, Verificación y Razonamiento sobre Agentes Inteligentes </p>\n",
    "<p style=\"text-align: center\"> EJERCICIOS </p>\n",
    "<p> </p>\n",
    "\n",
    "**Nombre del alumno:**\n",
    "- Kenny Jesús Flores Huamán\n",
    "\n",
    "<!-- \n",
    "Para generar la tabla de contenidos que se va a haber a continuación, se ha hecho uso del siguiente código\n",
    "URL : https://stackoverflow.com/questions/21151450/how-can-i-add-a-table-of-contents-to-a-jupyter-jupyterlab-notebook#:~:text=Click%20the%20toc2%20symbol%20in,you%20open%20it%20in%20JupyterLab.\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "def generate_toc(notebook_path, indent_char=\"&emsp;\"):\n",
    "    is_markdown = lambda it: \"markdown\" == it[\"cell_type\"]\n",
    "    is_title = lambda it: it.strip().startswith(\"#\") and it.strip().lstrip(\"#\").lstrip()\n",
    "    with open(notebook_path, 'r') as in_f:\n",
    "        nb_json = json.load(in_f)\n",
    "    for cell in filter(is_markdown, nb_json[\"cells\"]):\n",
    "        for line in filter(is_title, cell[\"source\"]):\n",
    "            line = line.strip()\n",
    "            indent = indent_char * (line.index(\" \") - 1)\n",
    "            title = line.lstrip(\"#\").lstrip()\n",
    "            url = urllib.parse.quote(title.replace(\" \", \"-\"))\n",
    "            out_line = f\"{indent}[{title}](#{url})<br>\\n\"\n",
    "            print(out_line, end=\"\")\n",
    "            \n",
    "generate_toc('C6_FloresHuaman.ipynb')\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07de12",
   "metadata": {},
   "source": [
    "# Introduccion\n",
    "\n",
    "En este notebook se van a realizar la resolución de los diferentes ejercicios mostrados en la página oficial de la asignatura.\n",
    "Todos los archivos que se van a importar que no sean librerías estándar de Python se encuentran en el mismo directorio de donde se encuentra este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bfab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05267b02",
   "metadata": {},
   "source": [
    "# VEHICULO EN PENDIENTE\n",
    "\n",
    "## Versión 1\n",
    "\n",
    "Consideremos un vehículo que opera en una pendiente y utiliza paneles solares para recargarse. Puede encontrarse en uno de los tres estados de la pendiente: alto, medio y bajo. Si hace girar sus ruedas, sube la pendiente en cada paso de tiempo (de bajo a medio o de medio a alto) o se mantiene alto. Si no gira las ruedas, desciende por la pendiente en cada paso de tiempo (de alto a medio o de medio a bajo) o se mantiene bajo. Hacer girar las ruedas consume una unidad de energía por paso de tiempo. Estando en la parte alta o media de la pendiente gana tres unidades de energía por paso de tiempo a través de los paneles solares, mientras que estando en la parte baja de la pendiente no gana nada de energía por paso de tiempo. El robot quiere ganar tanta energía como sea posible\n",
    "\n",
    "1. Representa gráficamente el MDP.\n",
    "2. Resuelve el MDP utilizando la iteración de valores con un factor de descuento de 0,8.\n",
    "3. Describe la política óptima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba690196",
   "metadata": {},
   "source": [
    "#### Importacion de las librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c921d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehiclesplopeV1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = VehicleSlopeV1(discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393d999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7fe8dd9",
   "metadata": {},
   "source": [
    "## Versión 2 \n",
    "\n",
    "Supongamos ahora que el vehículo puede encontrarse en uno de los cuatro estados de la pendiente: superior, alto, medio y bajo. Si gira sus ruedas lentamente, sube la pendiente en cada paso de tiempo (de bajo a medio, o de medio a alto, o de alto a superior) con una probabilidad de 0,3\n",
    ", y desciende por la pendiente hasta la parte baja con una probabilidad de 0,7\n",
    ". Si hace girar sus ruedas rápidamente, sube la pendiente en cada paso de tiempo (de bajo a medio, o de medio a alto, o de alto a superior) con una probabilidad de 0,7\n",
    ", y se desliza por la pendiente hasta llegar abajo con una probabilidad de 0,3\n",
    ". El giro lento de las ruedas consume una unidad de energía por paso de tiempo. Girar las ruedas rápidamente consume dos unidades de energía por unidad de tiempo. El vehículo se encuentra en la parte baja de la pendiente y su objetivo es llegar a la cima con el mínimo consumo de energía previsto.\n",
    "\n",
    "1. Representa gráficamente el MDP.\n",
    "2. Resuelve el MDP utilizando iteración de valor sin descuento (es decir, iteración de valor con un factor de descuento de 1).\n",
    "3. Describe la política óptima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5d6e3",
   "metadata": {},
   "source": [
    "### Importacion de las librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehiclesplopeV2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb0a23",
   "metadata": {},
   "source": [
    "### Instanciación del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5548878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a instanciando con un valor del factor de descuento de 1 \n",
    "vehicle2 = VehicleSlopeV2(discount_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7217eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = TabularValueFunction()\n",
    "ValueIteration(vehicle2 values).value_iteration(max_iterations=100)\n",
    "policy = values.extract_policy(vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d67b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|  Estado  |     Acción    |\n",
      "+----------+---------------+\n",
      "| LOW      | SPIN_FAST     |\n",
      "+----------+---------------+\n",
      "| MEDIUM   | SPIN_FAST     |\n",
      "+----------+---------------+\n",
      "| HIGH     | SPIN_FAST     |\n",
      "+----------+---------------+\n",
      "| TOP      | SPIN_LOW      |\n",
      "+----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "policy.print_policy_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75b6d6",
   "metadata": {},
   "source": [
    "# MUNDO MALLA \n",
    "\n",
    "El ejemplo de mundo malla es una idealización del movimiento de un robot en un entorno. En cada momento, el robot se encuentra en una ubicación y puede desplazarse a las ubicaciones vecinas, recogiendo recompensas y castigos. Supongamos que las acciones son estocásticas, de modo que existe una distribución de probabilidad sobre los estados resultantes dada la acción y el estado.\n",
    "\n",
    "![Descripción de la imagen](grid.png)\n",
    "\n",
    "\n",
    "En la figura adjunta se muestra un mundo cuadriculado de 10×10\n",
    ", en el que el robot puede elegir una de cuatro acciones: arriba, abajo, izquierda o derecha. Si el agente lleva a cabo una de estas acciones, tiene una probabilidad de 0.7\n",
    " de dar un paso en la dirección deseada y una probabilidad de 0.1\n",
    " de dar un paso en cualquiera de las otras tres direcciones. Si choca contra la pared exterior (es decir, la ubicación calculada está fuera de la malla), tiene una penalización de 1\n",
    " (es decir, una recompensa de −1\n",
    ") y el agente no se mueve. Hay cuatro estados con recompensa (aparte de las paredes): +10\n",
    " (en la posición (9,8)\n",
    "), +3\n",
    " (en (8,3)\n",
    "), −5\n",
    " (en (4,5)\n",
    ") y −10\n",
    " (en (4,8)\n",
    "). En cada uno de estos estados, el agente obtiene la recompensa después de realizar una acción en ese estado, no cuando entra en él. Cuando el agente alcanza uno de los estados con recompensa positiva (ya sea +3\n",
    " o +10\n",
    "), independientemente de la acción que realice, en el siguiente paso es lanzado, al azar, a una de las cuatro esquinas del mundo cuadriculado.\n",
    "\n",
    "1. Representa gráficamente el MDP.\n",
    "2. Resuelve el MDP por diversos métodos.\n",
    "3. Describe la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c0bf5",
   "metadata": {},
   "source": [
    "## Representación del MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc84d0e",
   "metadata": {},
   "source": [
    "### Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc32734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gridworld import *\n",
    "from policy_iteration import PolicyIteration\n",
    "from tabular_policy import TabularPolicy\n",
    "from qtable import QTable\n",
    "from qlearning import QLearning\n",
    "from sarsa import SARSA\n",
    "from multi_armed_bandit import EpsilonGreedy\n",
    "from tabular_value_function import TabularValueFunction\n",
    "from value_iteration import ValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec1792",
   "metadata": {},
   "source": [
    "### Instancia del mundo malla 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d955c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld = GridWorld(goals=[((8, 2), +10), ((7, 7), +3),\n",
    "                     ((3, 5), -5), ((3, 2), -10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43b234",
   "metadata": {},
   "source": [
    "### Representacion del estado inicial de la malla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a54a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny/Documentos/PyCA/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "gridworld.visualise_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710b965",
   "metadata": {},
   "source": [
    "## Resolucion del MDP por diversos métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb5919",
   "metadata": {},
   "source": [
    "### Resolucion del MDP mediante Iteración de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0b3316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  →  | 3  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  ↑  |  →  |  →  |  →  |  →  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  ←  | -5  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  →  |  ↓  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  →  |  ↑  |  →  |  →  |  →  |  →  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  ←  | -10  |  →  |  →  |  →  |  →  | 10  |  ← \n",
      " |  ↓  |  →  |  →  |  ↓  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = TabularValueFunction()\n",
    "ValueIteration(gridworld, values).value_iteration(max_iterations=100)\n",
    "policy = values.extract_policy(gridworld)\n",
    "print(gridworld.policy_to_string(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca21e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "| Estado          | Accion     |\n",
      "+------------------------------+\n",
      "| ('end', 'end')  | end        |\n",
      "+------------------------------+\n",
      "| (0, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (0, 1)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (0, 2)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (0, 3)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (0, 4)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (0, 5)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (0, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (0, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (0, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (0, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 2)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 4)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 5)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (1, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 2)          | LEFT       |\n",
      "+------------------------------+\n",
      "| (2, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 4)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 5)          | LEFT       |\n",
      "+------------------------------+\n",
      "| (2, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (2, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (3, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (3, 1)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (3, 2)          | end        |\n",
      "+------------------------------+\n",
      "| (3, 3)          | UP         |\n",
      "+------------------------------+\n",
      "| (3, 4)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (3, 5)          | end        |\n",
      "+------------------------------+\n",
      "| (3, 6)          | UP         |\n",
      "+------------------------------+\n",
      "| (3, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (3, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (3, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 2)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 4)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 5)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (4, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 2)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 4)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 5)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 8)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (5, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 2)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 4)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 5)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 7)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (6, 8)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (6, 9)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 0)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 1)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 2)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 3)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 4)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (7, 5)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (7, 6)          | RIGHT      |\n",
      "+------------------------------+\n",
      "| (7, 7)          | end        |\n",
      "+------------------------------+\n",
      "| (7, 8)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (7, 9)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 0)          | UP         |\n",
      "+------------------------------+\n",
      "| (8, 1)          | UP         |\n",
      "+------------------------------+\n",
      "| (8, 2)          | end        |\n",
      "+------------------------------+\n",
      "| (8, 3)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 4)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 5)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 6)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 7)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 8)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (8, 9)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 0)          | UP         |\n",
      "+------------------------------+\n",
      "| (9, 1)          | UP         |\n",
      "+------------------------------+\n",
      "| (9, 2)          | LEFT       |\n",
      "+------------------------------+\n",
      "| (9, 3)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 4)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 5)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 6)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 7)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 8)          | DOWN       |\n",
      "+------------------------------+\n",
      "| (9, 9)          | DOWN       |\n",
      "+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "policy.print_policy_table(15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412c8c8",
   "metadata": {},
   "source": [
    "### Resolucion del MDP mediante Iteración de políticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702e41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  →  | 3  |  ↓  |  ↓ \n",
      " |  →  |  →  |  →  |  ↑  |  →  |  →  |  →  |  →  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  ←  | -5  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  →  |  ↓  |  →  |  →  |  →  |  ↓  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  →  |  ↑  |  →  |  →  |  →  |  →  |  ↓  |  ↓ \n",
      " |  ↓  |  →  |  ←  | -10  |  →  |  →  |  →  |  →  | 10  |  ← \n",
      " |  ↓  |  →  |  →  |  ↓  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = TabularPolicy(default_action=gridworld.UP)\n",
    "PolicyIteration(gridworld, policy).policy_iteration(max_iterations=300)\n",
    "print(gridworld.policy_to_string(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54c3df",
   "metadata": {},
   "source": [
    "### Resolucion del MDP mediante Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03fce93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|████████████████████████████| 2000/2000 [00:06<00:00, 316.10it/s]\n"
     ]
    }
   ],
   "source": [
    "qfunction = QTable()\n",
    "QLearning(gridworld, EpsilonGreedy(), qfunction).execute(episodes=2000)\n",
    "policy = qfunction.extract_policy(gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa11e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  |  ↓  |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ←  |  ↑ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  ↓  |  ←  |  ←  |  ↑ \n",
      " |  →  |  →  |  →  |  ↑  |  →  |  →  |  →  | 3  |  ←  |  ↑ \n",
      " |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑ \n",
      " |  ↑  |  ↑  |  ←  | -5  |  →  |  ↑  |  ↑  |  ←  |  ↑  |  ↑ \n",
      " |  →  |  ↑  |  ↑  |  ↓  |  ←  |  →  |  →  |  ↑  |  ↑  |  ↑ \n",
      " |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ←  |  ↓  |  ↑  |  ← \n",
      " |  ↑  |  ↑  |  ←  | -10  |  →  |  ↓  |  →  |  ↓  | 10  |  ↑ \n",
      " |  ↑  |  ↑  |  ↑  |  ↓  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑ \n",
      " |  ↑  |  ←  |  ↑  |  ←  |  ↑  |  ←  |  ↑  |  ↑  |  ↑  |  ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridworld.policy_to_string(policy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654290db",
   "metadata": {},
   "source": [
    "### Resolucion del MDP mediante Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b08275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|████████████████████████████| 3000/3000 [00:04<00:00, 644.59it/s]\n"
     ]
    }
   ],
   "source": [
    "qfunction = QTable()\n",
    "SARSA(gridworld, EpsilonGreedy(), qfunction).execute(episodes=3000)\n",
    "policy = qfunction.extract_policy(gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57d2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  |  ↑  |  →  |  →  |  →  |  →  |  →  |  →  |  ←  |  ←  |  ↑ \n",
      " |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  →  |  ↓  |  ↓  |  ←  |  ↑ \n",
      " |  ↑  |  →  |  ↑  |  ↑  |  ↑  |  ↑  |  →  | 3  |  ↑  |  ↑ \n",
      " |  ↑  |  ←  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑ \n",
      " |  ←  |  ←  |  ←  | -5  |  →  |  ↑  |  ↑  |  ↑  |  ↑  |  ↑ \n",
      " |  ←  |  ←  |  ←  |  ↓  |  ↑  |  →  |  ↑  |  ←  |  ↓  |  ↓ \n",
      " |  →  |  ←  |  ↓  |  ↑  |  ↓  |  →  |  ↓  |  →  |  →  |  ↓ \n",
      " |  ↓  |  ←  |  ←  | -10  |  →  |  →  |  →  |  →  | 10  |  ← \n",
      " |  ↓  |  ↓  |  ↓  |  ↓  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      " |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  →  |  ↑  |  ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridworld.policy_to_string(policy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bbb35",
   "metadata": {},
   "source": [
    "## Descripción de la política óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18c5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3766282",
   "metadata": {},
   "source": [
    "# CartPole\n",
    "\n",
    "Este problema, también llamado a veces problema de equilibrio de pértiga, permite que el agente mueva un carro horizontalmente. Como se muestra en la figura, el carro tiene una pértiga rígida unida a él por una rótula, de modo que cuando el carro se mueve hacia la derecha o izquierda, la pértiga puede girar alrededor de la unión. El objetivo es mantener el poste verticalmente equilibrado mientras se mantiene el carro dentro de los límites laterales permitidos. Se obtiene 1\n",
    " de recompensa cada unidad de tiempo en el que la pértiga no supera cierto ángulo de inclinación y el carro se encuentra dentro de ciertos límites laterales, y la transición a un estado terminal de recompensa 0\n",
    " se produce en caso contrario.\n",
    "\n",
    "Las acciones consisten en aplicar una fuerza F\n",
    " a la izquierda o a la derecha del carro. El espacio de estados está definido por cuatro variables continuas: la posición lateral del carro x\n",
    ", su velocidad lateral v\n",
    ", el ángulo de la pértiga θ\n",
    " y la velocidad angular de la pértiga w\n",
    ". Además, el problema involucra una variedad de parámetros necesarios para el modelado físico, que incluyen: la masa del carro mcart\n",
    ", la masa de la pértiga mpole\n",
    ", la longitud de la pértiga l\n",
    ", la magnitud de la fuerza |F|\n",
    ", la aceleración gravitacional g\n",
    ", el paso de tiempo Δt\n",
    ", la desviación x\n",
    " máxima, la desviación angular máxima, y las pérdidas por fricción entre el carro y la pértiga o entre el carro y la pista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab170cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f9996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79b440cd",
   "metadata": {},
   "source": [
    "# Mountain Car\n",
    "\n",
    "En el problema del coche de montaña un vehículo debe conducir hacia la derecha para salir de un valle. Las paredes del valle son lo suficientemente empinadas como para que acelerar a ciegas hacia la meta con una velocidad insuficiente haga que el vehículo se detenga y se deslice hacia abajo. El agente debe aprender a acelerar primero hacia la izquierda para ganar suficiente impulso en el retorno y poder subir la colina.\n",
    "\n",
    "El estado es la posición horizontal del vehículo x∈[−1.2,0.6]\n",
    " y la velocidad v∈[−0.07,0.07]\n",
    ". En cualquier paso de tiempo, el vehículo puede acelerar a la izquierda (a=−1\n",
    "), acelerar a la derecha (a=1\n",
    "), o dejarse llevar por la inercia (a=0\n",
    "). Recibimos una recompensa de −1\n",
    " en cada cambio de sentido, y terminamos cuando el vehículo llega al lado derecho del valle más allá de x=0.6\n",
    ".\n",
    "\n",
    "La figura adjunta muestra una visualización animada de una ejecución del problema.\n",
    "\n",
    "Las transiciones en el problema del coche de montaña son deterministas:\n",
    "\n",
    "v′x′←←v+0.001 a−0.0025cos(3x)x+v′\n",
    "El término gravitacional en la actualización de la velocidad es lo que impulsa al vehículo con poca potencia hacia el fondo del valle. Las transiciones se sujetan a los límites del espacio de estados.\n",
    "\n",
    "El problema del coche de montaña es un buen ejemplo de un problema con retorno retardado. Se requieren muchas acciones para llegar al estado objetivo, lo que dificulta que un agente no entrenado reciba algo más que penalizaciones unitarias de forma continua. Los mejores algoritmos de aprendizaje son capaces de propagar eficazmente el conocimiento de las trayectorias que llegan a la meta al resto del espacio de estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b77ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mountaincar import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
